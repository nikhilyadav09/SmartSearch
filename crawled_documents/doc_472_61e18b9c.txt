A metasearch engine (or search aggregator) is an online information retrieval tool that uses the data of a web search engine to produce its own results.[1][2] Metasearch engines take input from a user and immediately query search engines[3] for results. Sufficient data is gathered, ranked, and presented to the users. Problems such as spamming reduce the accuracy and precision of results.[4] The process of fusion aims to improve the engineering of a metasearch engine.[5] Examples of metasearch engines include Skyscanner and Kayak.com, which aggregate search results of online travel agencies and provider websites. SearXNG is a generic free and open-source search software which aggregates results from internet search engines and other sources like Wikipedia and is offered for free by more than 70 SearxNG providers.[6] History The first person to incorporate the idea of meta searching was University of Washington student Eric Selberg,[7] who published a paper about his MetaCrawler experiment in 1995. The search engine is still usable as of 2024.[8] On May 20, 1996, HotBot, then owned by Wired, was a search engine with search results coming from the Inktomi and Direct Hit databases. It was known for its fast results and as a search engine with the ability to search within search results. Upon being bought by Lycos in 1998, development for the search engine staggered and its market share fell drastically. After going through a few alterations, HotBot was redesigned into a simplified search interface, with its features being incorporated into Lycos' website redesign.[9] In 1997, Daniel Dreilinger published a paper on his experimental metasearch engine, SavvySearch, which was able to automatically select the correct search engine to prioritize based on prior experience.[10] A metasearch engine called Anvish was developed by Bo Shu and Subhash Kak in 1999; the search results were sorted using instantaneously trained neural networks.[11] This was later incorporated into another metasearch engine called Solosearch.[12] In August 2000, India got its first meta search engine when HumHaiIndia.com was launched.[13] It was developed by the then 16 year old Sumeet Lamba.[14] The website was later rebranded as Tazaa.com.[15] Ixquick is a search engine known for its privacy policy statement. Developed and launched in 1998 by David Bodnick, it is owned by Surfboard Holding BV. In June 2006, Ixquick began to delete private details of its users following the same process with Scroogle. Ixquick's privacy policy includes no recording of users' IP addresses, no identifying cookies, no collection of personal data, and no sharing of personal data with third parties.[16] It also uses a unique ranking system where a result is ranked by stars. The more stars in a result, the more search engines agreed on the result. In April 2005, Dogpile, then owned and operated by InfoSpace, Inc., collaborated with researchers from the University of Pittsburgh and Pennsylvania State University to measure the overlap and ranking differences of leading Web search engines in order to gauge the benefits of using a metasearch engine to search the web. Results found that from 10,316 random user-defined queries from Google, Yahoo!, and Ask Jeeves, only 3.2% of first page search results were the same across those search engines for a given query. Another study later that year using 12,570 random user-defined queries from Google, Yahoo!, MSN Search, and Ask Jeeves found that only 1.1% of first page search results were the same across those search engines for a given query.[17] Advantages By sending multiple queries to several other search engines this extends the coverage data of the topic and allows more information to be found. They use the indexes built by other search engines, aggregating and often post-processing results in unique ways. A metasearch engine has an advantage over a single search engine because more results can be retrieved with the same amount of exertion.[2] It also reduces the work of users from having to individually type in searches from different engines to look for resources.[2] Metasearching is also a useful approach if the purpose of the user's search is to get an overview of the topic or to get quick answers. Instead of having to go through multiple search engines like Yahoo! or Google and comparing results, metasearch engines are able to quickly compile and combine results. They can do it either by listing results from each engine queried with no additional post-processing (Dogpile) or by analyzing the results and ranking them by their own rules (IxQuick, Metacrawler, and Vivismo). A metasearch engine can also hide the searcher's IP address from the search engines queried thus providing privacy to the search. Disadvantages Metasearch engines are not capable of parsing query forms or able to fully translate query syntax. The number of hyperlinks generated by metasearch engines are limited, and therefore do not provide the user with the complete results of a query.[18] The majority of metasearch engines do not provide over ten linked files from a single search engine, and generally do not interact with larger search engines for results. Pay per click links are prioritised and are normally displayed first.[19] Metasearching also gives the illusion that there is more coverage of the topic queried, particularly if the user is searching for popular or commonplace information. It's common to end with multiple identical results from the queried engines. It is also harder for users to search with advanced search syntax to be sent with the query, so results may not be as precise as when a user is using an advanced search interface at a specific engine. This results in many metasearch engines using simple searching.[20] Operation A metasearch engine accepts a single search request from the user. This search request is then passed on to another search engine's database. A metasearch engine does not create a database of web pages but generates a Federated database system of data integration from multiple sources.[21][22][23] Since every search engine is unique and has different algorithms for generating ranked data, duplicates will therefore also be generated. To remove duplicates, a metasearch engine processes this data and applies its own algorithm. A revised list is produced as an output for the user.[citation needed] When a metasearch engine contacts other search engines, these search engines will respond in three ways: Architecture of ranking Web pages that are highly ranked on many search engines are likely to be more relevant in providing useful information.[24] However, all search engines have different ranking scores for each website and most of the time these scores are not the same. This is because search engines prioritise different criteria and methods for scoring, hence a website might appear highly ranked on one search engine and lowly ranked on another. This is a problem because Metasearch engines rely heavily on the consistency of this data to generate reliable accounts.[24] Fusion A metasearch engine uses the process of Fusion to filter data for more efficient results. The two main fusion methods used are: Collection Fusion and Data Fusion. Spamdexing Spamdexing is the deliberate manipulation of search engine indexes. It uses a number of methods to manipulate the relevance or prominence of resources indexed in a manner unaligned with the intention of the indexing system. Spamdexing can be very distressing for users and problematic for search engines because the return contents of searches have poor precision.[citation needed] This will eventually result in the search engine becoming unreliable and not dependable for the user. To tackle Spamdexing, search robot algorithms are made more complex and are changed almost every day to eliminate the problem.[27] It is a major problem for metasearch engines because it tampers with the Web crawler's indexing criteria, which are heavily relied upon to format ranking lists. Spamdexing manipulates the natural ranking system of a search engine, and places websites higher on the ranking list than they would naturally be placed.[28] There are three primary methods used to achieve this: Content spam Content spam are the techniques that alter the logical view that a search engine has over the page's contents. Techniques include: Link spam Link spam are links between pages present for reasons other than merit. Techniques include: Cloaking This is an SEO technique in which different materials and information are sent to the web crawler and to the web browser.[29] It is commonly used as a spamdexing technique because it can trick search engines into either visiting a site that is substantially different from the search engine description or giving a certain site a higher ranking. See also References